{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d62d23a6",
   "metadata": {},
   "source": [
    "## Part of Speech (POS) Tagging using Hidden Markov Model\n",
    "#### This Activaty is Made by Mark Andrei Encanto and Ethan Gabriel Soncio for our NLP Course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605cb91c",
   "metadata": {},
   "source": [
    "\n",
    "1. Importing Libraries\n",
    "2. Dataset Description\n",
    "3. Training the Model\n",
    "4. Testing and Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99d6f69",
   "metadata": {},
   "source": [
    "### Importing Libraries\n",
    "We will import the necessary libraries for implementing the Hidden Markov Model (HMM). These libraries include:\n",
    "- `nltk` for natural language processing tasks, including tokenization and tagging.\n",
    "- `defaultdict` from the `collections` module will help us efficiently manage and count occurrences of tags and words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca226260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import treebank\n",
    "from nltk.corpus import conll2000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc510d7",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "We will use data set from the NLTK library:\n",
    "- `treebank`, `brown`, `conll2000` dataset that contains part-of-speech tagged sentences\n",
    "- `universal_tagset` for simplified POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7ae3d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to C:\\Users\\Mark Andrei\n",
      "[nltk_data]     Encanto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to C:\\Users\\Mark\n",
      "[nltk_data]     Andrei Encanto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data] Downloading package brown to C:\\Users\\Mark Andrei\n",
      "[nltk_data]     Encanto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n",
      "[nltk_data] Downloading package universal_tagset to C:\\Users\\Mark\n",
      "[nltk_data]     Andrei Encanto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package conll2000 to C:\\Users\\Mark Andrei\n",
      "[nltk_data]     Encanto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data] Downloading package universal_tagset to C:\\Users\\Mark\n",
      "[nltk_data]     Andrei Encanto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For treebank corpus\n",
    "nltk.download('treebank')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "# For brown corpus\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "# For conll2000 corpus\n",
    "nltk.download('conll2000')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ef8d424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "treebank_corpus = treebank.tagged_sents(tagset='universal')\n",
    "brown_corpus = brown.tagged_sents(tagset='universal')\n",
    "conll_corpus = conll2000.tagged_sents(tagset='universal')\n",
    "sentences = treebank_corpus + brown_corpus + conll_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7444ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "transition = defaultdict(lambda: defaultdict(int))\n",
    "emission = defaultdict(lambda: defaultdict(int))\n",
    "start_tag_count = defaultdict(int)\n",
    "tag_count = defaultdict(int)\n",
    "\n",
    "for sent in sentences:\n",
    "    prev_tag = None\n",
    "    for i, (word, tag) in enumerate(sent):\n",
    "        word = word.lower()\n",
    "        emission[tag][word] += 1\n",
    "        tag_count[tag] += 1\n",
    "        if i == 0:\n",
    "            start_tag_count[tag] += 1\n",
    "        if prev_tag:\n",
    "            transition[prev_tag][tag] += 1\n",
    "        prev_tag = tag\n",
    "\n",
    "def normalize(d):\n",
    "    total = sum(d.values())\n",
    "    return {k: v / total for k, v in d.items()}\n",
    "\n",
    "start_prob = normalize(start_tag_count)\n",
    "transition_prob = {t1: normalize(t2) for t1, t2 in transition.items()}\n",
    "emission_prob = {tag: normalize(words) for tag, words in emission.items()}\n",
    "all_tags = list(tag_count.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59f3ea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(words, tags, start_p, trans_p, emit_p):\n",
    "    V = [{}]\n",
    "    path = {}\n",
    "\n",
    "    for tag in tags:\n",
    "        V[0][tag] = start_p.get(tag, 1e-6) * emit_p.get(tag, {}).get(words[0].lower(), 1e-6)\n",
    "        path[tag] = [tag]\n",
    "\n",
    "    for t in range(1, len(words)):\n",
    "        V.append({})\n",
    "        new_path = {}\n",
    "        for curr_tag in tags:\n",
    "            prob, prev_tag = max((V[t-1][pt] * trans_p.get(pt, {}).get(curr_tag, 1e-6) *\n",
    "                                  emit_p.get(curr_tag, {}).get(words[t].lower(), 1e-6), pt)\n",
    "                                 for pt in tags)\n",
    "            V[t][curr_tag] = prob\n",
    "            new_path[curr_tag] = path[prev_tag] + [curr_tag]\n",
    "        path = new_path\n",
    "\n",
    "    prob, final_tag = max((V[-1][tag], tag) for tag in tags)\n",
    "    return path[final_tag]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dbfbba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM POS Tagging Accuracy: 0.9508\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           .       1.00      1.00      1.00       315\n",
      "         ADJ       0.90      0.96      0.93       167\n",
      "         ADP       0.91      0.94      0.92       250\n",
      "         ADV       0.93      0.91      0.92        55\n",
      "        CONJ       1.00      1.00      1.00        59\n",
      "         DET       0.85      0.98      0.91       205\n",
      "        NOUN       0.99      0.96      0.97       779\n",
      "         NUM       1.00      1.00      1.00       151\n",
      "        PRON       1.00      0.53      0.70        58\n",
      "         PRT       0.76      0.76      0.76        78\n",
      "        VERB       0.96      0.97      0.96       321\n",
      "\n",
      "    accuracy                           0.95      2438\n",
      "   macro avg       0.94      0.91      0.92      2438\n",
      "weighted avg       0.95      0.95      0.95      2438\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_sentences = sentences[-100:]\n",
    "X_test = [[w for w, t in sent] for sent in test_sentences]\n",
    "y_true = [[t for w, t in sent] for sent in test_sentences]\n",
    "\n",
    "y_pred = []\n",
    "for sent in X_test:\n",
    "    pred_tags = viterbi(sent, all_tags, start_prob, transition_prob, emission_prob)\n",
    "    y_pred.append(pred_tags)\n",
    "\n",
    "y_true_flat = [tag for sent in y_true for tag in sent]\n",
    "y_pred_flat = [tag for sent in y_pred for tag in sent]\n",
    "\n",
    "acc = accuracy_score(y_true_flat, y_pred_flat)\n",
    "print(f\"HMM POS Tagging Accuracy: {acc:.4f}\")\n",
    "\n",
    "print(classification_report(y_true_flat, y_pred_flat, zero_division=0))\n",
    "\n",
    "labels = sorted(list(set(y_true_flat + y_pred_flat)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
